# -*- coding: utf-8 -*-
"""Wallmart_sales_forecast (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RIk-K8Rv6v-HD6v2RExtx_c9teLKxyeq

![Sales_forecast](https://www.planplusonline.com/wp-content/uploads/2015/03/sales-forecast.png)
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
from plotly.offline import iplot

from sklearn.model_selection import train_test_split
from math import sqrt
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import warnings

# from google.colab import drive
# drive.mount('/content/drive')

features=pd.read_csv('/content/features.csv',low_memory=True)
stores=pd.read_csv('/content/stores.csv',low_memory=True)
test=pd.read_csv('/content/test.csv',low_memory=True)
train=pd.read_csv('/content/train.csv',low_memory=True)

train.head()

train.info()

train['Date']=pd.to_datetime(train['Date'])

train.describe()

train.isna().sum()

features

features.info()

features['Date']=pd.to_datetime(features['Date'])

features.describe()

features['Temperature'].plot(kind='kde')

features.isna().sum()

stores.head()

stores.isna().sum()

stores['Type'].value_counts().plot(kind='pie',autopct='%.2f%%',title='Store Type Distribution')
plt.show()

"""## `Combine features and stores dataset`"""

df=pd.merge(features,stores,how='inner',on='Store')
df.head()

df.info()

df.describe()

test

px.pie(train,names=train['Dept'].value_counts().sort_values(ascending=False).head(10).index,values=train['Dept'].value_counts().sort_values(ascending=False).head(10).values,title='top 10 Department')

train['Store'].value_counts().sort_values(ascending=False).head(10).plot(kind='pie',autopct='%.1f%%',figsize=(12,6))

px.pie(train,names=train['Store'].value_counts().sort_values(ascending=False).head(10).index,values=train['Store'].value_counts().sort_values(ascending=False).head(10).values,title='Top 10 stores')

px.pie(train,names=train['IsHoliday'].value_counts().sort_values(ascending=False).head(10).index,values=train['IsHoliday'].value_counts().sort_values(ascending=False).head(10).values)

test.info()

test['Date']=pd.to_datetime(test['Date'])

test.describe()

df.info()

df['Week']=df['Date'].dt.isocalendar().week
df['Year']=df['Date'].dt.year

df.head()

train_merge=pd.merge(train,df,on=['Store','Date','IsHoliday']).sort_values(by=['Store','Date','IsHoliday'])
train_merge

test_merge=pd.merge(test,df,on=['Store','Date','IsHoliday']).sort_values(by=['Store','Date','IsHoliday'])
test_merge

import pandas as pd
import plotly.express as px

# Function to create scatter plots
def scatter(train_merge, column):
    fig = px.scatter(
        train_merge,
        x=column,
        y='Weekly_Sales',
        title=f'Scatter Plot of Weekly Sales vs. {column}',
        labels={
            column: column,  # X-axis label
            'Weekly_Sales': 'Weekly Sales'  # Y-axis label
        }
    )
    fig.show()

# Example usage with different columns
scatter(train_merge, 'Fuel_Price')  # with respect to Fuel_Price

scatter(train_merge, 'Size')  # with respect to Size

scatter(train_merge, 'CPI')  # with respect to CPI

scatter(train_merge, 'Type')  # with respect to Type

scatter(train_merge, 'IsHoliday')  # with respect to IsHoliday

scatter(train_merge, 'Unemployment')  # with respect to Unemployment

scatter(train_merge, 'Temperature')  # with respect to Temperature

scatter(train_merge, 'Store')  # with respect to Store

scatter(train_merge, 'Dept')  # with respect to Dept

train_merge

train_merge[train_merge['Year']==2010]['Weekly_Sales'].groupby(train_merge['Week']).mean().plot()

train_merge[train_merge['Year']==2011]['Weekly_Sales'].groupby(train_merge['Week']).mean().plot()

train_merge[train_merge['Year']==2010]['Weekly_Sales'].groupby(train_merge['Week']).mean().plot(label='2010')
train_merge[train_merge['Year']==2011]['Weekly_Sales'].groupby(train_merge['Week']).mean().plot(label='2011')
train_merge[train_merge['Year']==2012]['Weekly_Sales'].groupby(train_merge['Week']).mean().plot(label='2012')
plt.legend(title='Year')
plt.show()

temp_df=train_merge.groupby('Dept')['Weekly_Sales'].mean().reset_index()
temp_df

fig=px.bar(temp_df,x='Dept',y='Weekly_Sales',color='Dept',text_auto=True)
fig.update_layout(
    xaxis_title='Department',
    yaxis_title='Average Weekly Sales'
)

temp_df=train_merge.groupby('Store')['Weekly_Sales'].mean().reset_index()
temp_df

fig=px.bar(temp_df,x='Store',y='Weekly_Sales',color='Store',text_auto=True)
fig.update_layout(
    xaxis_title='Store',
    yaxis_title='Average Weekly Sales'
)

temp_df=train_merge.select_dtypes('number')

corr=temp_df.corr()

mask=np.triu(corr)

mask

plt.figure(figsize=(15,12),dpi=100)
sns.heatmap(corr,mask=mask,annot=True)
plt.show()

"""##`Droping column with weak corr`"""

train_merge.columns

train_merge.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',
       'MarkDown5','Fuel_Price'],inplace=True)
test_merge.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',
       'MarkDown5','Fuel_Price'],inplace=True)

train_merge.head()

test_merge.head()

"""## `Dividing train and test dataset`"""

X=train_merge[['Store', 'Dept', 'IsHoliday','Size', 'Week', 'Year']]
y=train_merge['Weekly_Sales']

y.info()

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

"""# `Performing GridsearchCV on Ridge Regression`"""

params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 108000]}
ridge_regressor = GridSearchCV(Ridge(), params, cv=7, scoring='neg_mean_absolute_error', n_jobs=-1)

try:
    ridge_regressor.fit(X_train, y_train)
except Exception as e:
    print(f"An error occurred: {e}")

print("Best parameters found:", ridge_regressor.best_params_)
print("Best score found:", ridge_regressor.best_score_)

y_train_pred=ridge_regressor.predict(X_train)
y_test_pred=ridge_regressor.predict(X_test)

print(len(y_train_pred))
print(len(y_test_pred))

# For training data
print("Training MSE:", mse(y_train, y_train_pred))
print("Training RMSE:", np.sqrt(mse(y_train, y_train_pred)))
print("Training R^2 Score:", r2_score(y_train, y_train_pred))

# For test data
print("Test MSE:", mse(y_test, y_test_pred))
print("Test RMSE:", np.sqrt(mse(y_test, y_test_pred)))
print("Test R^2 Score:", r2_score(y_test, y_test_pred))

params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}

# Initialize GridSearchCV with Lasso model
lasso_regressor = GridSearchCV(Lasso(), params, cv=7, scoring='neg_mean_absolute_error', n_jobs=-1)

# Fit the model


try:
    lasso_regressor.fit(X_train, y_train)
except Exception as e:
    print(f"An error occurred: {e}")

print("Best parameters found:", lasso_regressor.best_params_)
print("Best score found:", lasso_regressor.best_score_)

y_train_pred=lasso_regressor.predict(X_train)
y_test_pred=lasso_regressor.predict(X_test)

# For training data
print("Training MSE:", mse(y_train, y_train_pred))
print("Training RMSE:", np.sqrt(mse(y_train, y_train_pred)))
print("Training R^2 Score:", r2_score(y_train, y_train_pred))

# For test data
print("Test MSE:", mse(y_test, y_test_pred))
print("Test RMSE:", np.sqrt(mse(y_test, y_test_pred)))
print("Test R^2 Score:", r2_score(y_test, y_test_pred))

from sklearn.ensemble import RandomForestRegressor
tuned_params = {'n_estimators': [100, 200], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
random_regressor = RandomizedSearchCV(RandomForestRegressor(), tuned_params, n_iter = 3, scoring = 'neg_mean_absolute_error', cv = 3, n_jobs = -1)
random_regressor.fit(X_train, y_train)

y_train_pred=random_regressor.predict(X_train)
y_test_pred=random_regressor.predict(X_test)

y_test.head(10)

y_test_pred[:10]

# For training data
print("Training MSE:", mse(y_train, y_train_pred))
print("Training RMSE:", np.sqrt(mse(y_train, y_train_pred)))
print("Training R^2 Score:", r2_score(y_train, y_train_pred))

# For test data
print("Test MSE:", mse(y_test, y_test_pred))
print("Test RMSE:", np.sqrt(mse(y_test, y_test_pred)))
print("Test R^2 Score:", r2_score(y_test, y_test_pred))

"""## `**Conclusion**`
`With the above implementation it was possible to forecast the prediction for the Walmart Stores`
"""

